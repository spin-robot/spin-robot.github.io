<!DOCTYPE html>
<html>
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-BM130PWZB5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-BM130PWZB5');
  </script>
  <meta charset="utf-8">
  <meta name="description" content="SPIN: Simultaneous Perception, Interaction and Navigation">
  <meta name="keywords" content="Whole-body Coordination, Active Vision, Mobile Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SPIN-ROBOT</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

  <script src="https://www.youtube.com/iframe_api"></script>
  <script src="./static/js/ajax.googleapis.com_ajax_libs_jquery_3.5.1_jquery.min.js"></script>
  <script src="./static/js/isInViewport.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <link rel="icon" href="./favicon.ico?">

</head>
<body>

  <section class="hero">
    <div class="hero-body no-bottom-padding">
      <div class="container">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">SPIN: Simultaneous Perception, <br>Interaction and Navigation</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a target="_blank" href="https://shagunuppal.github.io/">Shagun Uppal</a>&nbsp;&nbsp;&nbsp;
                <a target="_blank" href="https://anag.me/">Ananye Agarwal</a>&nbsp;&nbsp;&nbsp;
                <a target="_blank" href="https://haoyu-x.github.io/">Haoyu Xiong</a>&nbsp;&nbsp;&nbsp;
                <a target="_blank" href="https://kennyshaw.net/">Kenneth Shaw</a>&nbsp;&nbsp;&nbsp;
                <a target="_blank" href="https://www.cs.cmu.edu/~dpathak/">Deepak Pathak</a>&nbsp;&nbsp;&nbsp;
                
                <!-- <br />Carnegie Mellon University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
                <br><br>
                <center>
                  <div class="image-row">
                    <img src="./resources/cmu-wordmark-horizontal-r.png" width="50%"></img>
                  </div>
                </center>
                
                <span class="brmod" style="color:rgb(183, 0, 0)"><b>CVPR 2024</b></span>
              </span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a target="_blank" href="https://arxiv.org/pdf/2405.07991.pdf"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- arXiv Link. -->
                <span class="link-block">
                  <a target="_blank" href="https://arxiv.org/abs/2405.07991"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <span class="link-block">
                  <!-- <a target="_blank" href="https://youtu.be/5sRqythe6TE" -->
                    <a target="_blank" href="https://spin-robot.github.io/"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
                <!-- twitter Link. -->
                <span class="link-block">
                  <!-- <a target="_blank" href="https://twitter.com/pathak2206/status/1592593398493761538" -->
                    <a target="_blank" href="https://spin-robot.github.io/"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-twitter"></i>
                    </span>
                    <span>Summary</span>
                  </a>
                </span>
              </div>
  
            </div>
            </div>
  
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

<section class="section" style="padding-bottom: 0">
    <div class="container">
    <div id="method_video" class="columns is-centered has-text-centered">
      <div class="column is-two-thirds no-bottom-padding">
        <!-- <video controls src="./resources/full_video/spin_final_video_v5_compressed.mov" style="border: 1px solid #bbb; border-radius: 10px; width: 100%;"> -->
          <!-- <iframe height="500" src="https://www.youtube.com/embed/RLF07HVEg_k" title="YouTube video player" frameborder="0" style="border: 1px solid #bbb; border-radius: 10px; width: 100%;" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
        
        <video width="960" height="720" loop="" muted="" controls="" picture-in-picture="" id="teaser">
          <source src="./resources/full_video/spin_final_video_v5_compressed.mov" type="video/mp4">
        </video>
      </div>
    </div>
  
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <br>
        <h2 class="title is-2">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            While there has been remarkable progress recently in the fields of manipulation and locomotion, mobile manipulation remains a long-standing challenge. Compared to locomotion or static manipulation, a mobile system must make a diverse range of long-horizon tasks feasible in unstructured and dynamic environments. While the applications are broad and interesting, there are a plethora of challenges in developing these systems such as coordination between the base and arm, reliance on onboard perception for perceiving and interacting with the environment and most importantly, simultaneously integrating all these parts together. Prior works approach the problem using disentangled modular skills for mobility and manipulation that are trivially tied together. This causes several limitations such as compounding errors, delays in decision-making and no whole-body coordination. In this work, we present a reactive mobile manipulation framework that uses an active visual system to consciously perceive and react to its environment. Similar to how humans leverage whole-body and hand-eye coordination, we develop a mobile manipulator that exploits its ability to move and see, more specifically -- to move in order to see and to see in order to move.  This allows it to not only move around and interact with its environment but also, choose <b><i>when</i></b> to perceive <b><i>what</i></b> using an active visual system.  We observe that such an agent learns to navigate around complex cluttered scenarios while displaying agile whole-body coordination using only ego-vision without needing to create environment maps.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<script>
  var mainLoopId = setInterval(function(){
        $('video').not('#teaser').each(function(){
      if ($(this).is(":in-viewport")) {
          $(this)[0].play();
      } else {
          $(this)[0].pause();
      }
      })
    },200);
</script>

<style>
  .thumbnails-container {
      display: flex;
      justify-content: center;
      flex-wrap: wrap;
  }

  .thumbnail {
      width: 30%;
      margin: 20px 1%;
      position: relative;
      overflow: hidden;
      border-radius: 10px;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.3);
      transition: none; /* Remove the transition effect */
  }

  .thumbnail img {
      width: 100%;
      height: auto;
      transition: none; /* Remove the transition effect */
  }

  .thumbnail-content {
      position: absolute;
      bottom: 0;
      left: 0;
      width: 100%;
      background-color: rgba(0, 0, 0, 0.7);
      padding: 20px;
      box-sizing: border-box;
      opacity: 1; /* Always visible */
  }

  .thumbnail-content h3 {
      color: #fff;
      margin: 0;
  }

  .thumbnail-content p {
      color: #fff;
      margin: 5px 0 0;
  }


  .heading-container {
    text-align: center;
  }
</style>
</head>
<body>

<div class="heading-container">
  <br>
  <h2 class="title is-2" style="padding-bottom: 10px;">Key Features</h2>
</div>

<div class="thumbnails-container">
  <div class="thumbnail">
    <img src="./resources/thumbnails/thumbnail_1.png" alt="Image 1">
    <div class="thumbnail-content">
      <h3><center>SPIN learns and <br> adapts on the move</center></h3>
      <!-- <p>SPIN learns to adapt on the move.</p> -->
    </div>
  </div>

  <div class="thumbnail">
    <img src="./resources/thumbnails/thumbnail_2.png" alt="Image 2">
    <div class="thumbnail-content">
      <h3><center>SPIN couples <br> action-perception optimization</center></h3>
      <!-- <p>SPIN couples perception and control optimization.</p> -->
    </div>
  </div>

  <div class="thumbnail">
    <img src="./resources/thumbnails/thumbnail_3.png" alt="Image 3">
    <div class="thumbnail-content">
      <h3><center>SPIN is trained with <br> large-scale randomization</center></h3>
      <!-- <p>SPIN shows robust in-the-wild generalization.</p> -->
    </div>
  </div>
</div>

<section class="section">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="column is-full-width">
          <h2 class="title is-4" style="text-align: center;">All videos play at 2x speed</h2>
        </div>
        <br><br>
        <h2 class="title is-2" style="text-align: center; padding-bottom: 10px;">Mobile Manipulation with Hand-Eye Coordination</h2>
        <tr>
          <td>
              <h2 class="subtitle">
                 SPIN looks around for finding the desired object using its actuated camera. Once the target object is located, the robot navigates towards the object to pick it up. Displaying hand-eye coordination, it adjust its base and arm to get a better grasp of the object by trying to keep the object within its field of view even while moving. <br>
              </h2>
          </td>
        </tr>
        
        <div id="method_video" class="columns is-centered has-text-centered">
          <div class="column is-two-thirds no-bottom-padding">
            <video playsinline autoplay loop muted src="./resources/hand-eye-coord/soft_toy.mov" style="border: 1px solid #bbb; border-radius: 10px; width: 100%;"></video><br><br>
          </div>
        </div>

        <tr>
          <td>
            <div class="row">
              <div class="columns is-centered">
                <div class="col">
                    <video playsinline autoplay loop muted src="./resources/hand-eye-coord/gum_box.mp4" style="border-radius: 10px;"></video>
                </div>
                <div class="col">
                  <video playsinline autoplay loop muted src="./resources/hand-eye-coord/coffee_cup.mov" style="border-radius: 10px;"></video>
                </div>
              </div>
            </div>
            <br>
          </td>
        </tr>
      </div> 
    </div>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-2" style="text-align: center; padding-bottom: 10px;">Emergent Whole-Body and Dynamic Obstacle Avoidance</h2>
        <tr>
          <td>
              <h2 class="subtitle">
                We observe emergent whole-body coordination for obstacle avoidance where the robot learns to move its arm to navigate across floating and dynamic obstacles efficiently without re-routing or re-planning base movement. Without any mapping or planning, active perception allows the robot to pan its head camera in order to aggregate useful information about its environment and dynamically adapt according to it. <br>
              </h2>
          </td>
        </tr>
        <tr>
          <td>
            <div class="row">
              <div class="columns is-centered">
                <div class="col">
                    <video playsinline autoplay loop muted src="./resources/whole_body/emergent_1.mp4" style="border-radius: 10px;"></video>
                </div>
                <div class="col">
                  <video playsinline autoplay loop muted src="./resources/whole_body/emergent_2.mov" style="border-radius: 10px;"></video>
                </div>
              </div>
            </div>
            <br>
          </td>
        </tr>
      </div> 
    </div>
  </div>
</section>


<section class="section">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full-width">  
        <h2 class="title is-2" style="text-align: center; padding-bottom: 10px;">SPIN cleans up the table</h2>
        <tr>
          <td>
              <h2 class="subtitle">
                SPIN can repeatedly fetch different objects and clean up a messy table. With an actuated camera head, whole-body coordination and robustness to random initializations, SPIN can seamlessly performs manipulation several times in a go. <br>
              </h2>
          </td>
        </tr>
        
        <div id="method_video" class="columns is-centered has-text-centered">
          <div class="column is-two-thirds no-bottom-padding">
            <video playsinline autoplay loop muted src="./resources/table_cleanup/grasping_clean_table_processed.mp4" style="border: 1px solid #bbb; border-radius: 10px; width: 100%;"></video><br><br>
          </div>
        </div>

      </div> 
    </div>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-2" style="text-align: center; padding-bottom: 10px;">Robustness to Adversarial Scenarios</h2>
        <tr>
          <td>
              <h2 class="subtitle">
                The policy shows robustness to a variety of adversarial conditions. In the fisrt video, we see an adversarial case of dynamic obstacle avoidance where humans actively try to block its path several times, but the robot continues to turn and re-route continously.
                <br><p style="margin:10px;"></p> In the second video, the robot nagivates in dimly-lit clutter in an outdoor environment. Even with the rough and bumpy floor which causes inaccuracies in the wheel odometry and jittery motion, the robot adjusts its movement.
              </h2>
          </td>
        </tr>
        <tr>
          <td>
            <div class="row">
              <div class="columns is-centered">
                <div class="col">
                    <video playsinline autoplay loop muted src="./resources/whole_body/emergent_3.mov" style="border-radius: 10px;"></video>
                </div>
                <div class="col">
                  <video playsinline autoplay loop muted src="./resources/whole_body/emergent_4.mp4" style="border-radius: 10px;"></video>
                </div>
              </div>
            </div>
            <br>
          </td>
        </tr>
      </div> 
    </div>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-2" style="text-align: center; padding-bottom: 10px;">Fetching a diverse set of objects</h2>
        <tr>
          <td>
              <h2 class="subtitle">
                  SPIN can fetch a diverse set of objects ranging from rigid to deformable of different shapes, sizes and masses, such as a plastic cup or a fruit or a soft toy. <br>
              </h2>
          </td>
        </tr>
        
        <tr>
          <td>
            <div class="row">
              <div class="columns is-centered">
                <div class="col">
                  <video playsinline autoplay loop muted src="./resources/grasping/toy_dice.mp4" width="100%"
                        style="border-radius:10px; "></video>
                </div>
                <div class="col">
                  <video playsinline autoplay loop muted src="./resources/grasping/banana.mp4" width="100%"
                        style="border-radius:10px; "></video>
                </div>
                <div class="col">
                  <video playsinline autoplay loop muted src="./resources/grasping/coffee_cup.mp4" width="100%"
                        style="border-radius:10px; "></video>
              </div>
              </div>
            </div>
            <br>
            <br>
            <div class="row">
              <div class="columns is-centered">
                <div class="col">
                    <video playsinline autoplay loop muted src="./resources/grasping/oil_bottle.mp4" width="100%"
                          style="border-radius:10px; "></video>
                </div>
                <div class="col">
                  <video playsinline autoplay loop muted src="./resources/grasping/inverted_cup.mp4" width="100%"
                        style="border-radius:10px; "></video>
                </div>
                <div class="col">
                  <video playsinline autoplay loop muted src="./resources/grasping/apple.mp4" width="100%"
                        style="border-radius:10px; "></video>
                </div>
              </div>
            </div>
            <br>
            <br>
            <div class="row">
              <div class="columns is-centered">
                <div class="col">
                    <video playsinline autoplay loop muted src="./resources/other/teddy_bear.mp4" width="100%"
                          style="border-radius:10px; "></video>
                </div>
                <div class="col">
                  <video playsinline autoplay loop muted src="./resources/other/carrot.mp4" width="100%"
                        style="border-radius:10px; "></video>
                </div>
                <div class="col">
                  <video playsinline autoplay loop muted src="./resources/other/soup_can.mp4" width="100%"
                        style="border-radius:10px; "></video>
                </div>
              </div>
            </div>
          </td>
        </tr>
      </div> 
    </div>
  </div>
</section>

<section class="section" id="BibTeX" style="margin-top: -200px;">
  <div class="container content">
    <h2 class="titile">BibTeX</h2>
    <pre><code>
      @misc{uppal2024spin,
        title={SPIN: Simultaneous Perception, Interaction and Navigation}, 
        author={Shagun Uppal and Ananye Agarwal and Haoyu Xiong and Kenneth Shaw and Deepak Pathak},
        year={2024},
        eprint={2405.07991},
        archivePrefix={arXiv},
        primaryClass={cs.RO}
    }
    </code></pre>
  </div>
</section>

<section class="section" id="Ack" style="margin-top: -50px;">
  <div class="container content">
    <h2 class="titile">Acknowledgements</h2>
    We thank Jared Mejia and Mihir Prabhudesai for helping with stress-testing in real-world experiments. We are also grateful to Zackory Erickson and the Hello Robot team for their support with the robot hardware. This work was supported in part by grants including ONR N00014-22-1-2096, AFOSR FA9550-23-1-0747, and the Google research award to DP.
  </div>
</section>

</body>
</html>
